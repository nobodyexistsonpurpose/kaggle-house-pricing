{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3514a346-f70e-4409-a0f0-b1913afb274f",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "In this notebook we will test OLS regressions but with additional data preprocessing and tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceae7e81-f841-4b5d-9bc7-c0fb1822df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from IPython.core.display import HTML\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from feature_engine.selection import DropCorrelatedFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "100ad498-e578-4b6b-b411-4a4ad1a578dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_without_nans.csv\")\n",
    "sub = pd.read_csv(\"data/test_without_nans.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f16b784-a0f9-451d-8850-3fc860d4159e",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ca79f-227c-4341-b841-e6dd17605f5a",
   "metadata": {},
   "source": [
    "## Categorical encoding and dummy variable trap\n",
    "\n",
    "In our encoded data we have dummy varible trap for the unregularized linear regression. On this purpose we will reencode our data for it independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "110ba2ce-4c29-4517-abbc-1bfc6b1c8e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [43] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_train = train.copy()\n",
    "encoded_sub = sub.copy()\n",
    "\n",
    "## Taking categorical and continuous columns\n",
    "object_columns = train.select_dtypes(include='object')\n",
    "object_columns[\"MSSubClass\"] = train[\"MSSubClass\"]\n",
    "continuous_columns = train.select_dtypes(exclude=\"object\").drop([\"SalePrice\", \"Id\", \"MSSubClass\"], axis=1)\n",
    "\n",
    "object_columns = object_columns.columns.values\n",
    "continuous_columns = continuous_columns.columns.values\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\", drop=\"first\")        \n",
    "one_hot_encoded_train = encoder.fit_transform(encoded_train[object_columns])\n",
    "one_hot_encoded_sub = encoder.transform(encoded_sub[object_columns])\n",
    "        \n",
    "one_hot_df_train = pd.DataFrame(one_hot_encoded_train,\n",
    "                                columns=encoder.get_feature_names_out(object_columns))\n",
    "one_hot_df_sub= pd.DataFrame(one_hot_encoded_sub,\n",
    "                                columns=encoder.get_feature_names_out(object_columns))\n",
    "        \n",
    "encoded_train = pd.concat([encoded_train, one_hot_df_train], axis=1)\n",
    "encoded_train = encoded_train.drop(object_columns, axis=1)\n",
    "\n",
    "encoded_sub = pd.concat([encoded_sub, one_hot_df_sub], axis=1)\n",
    "encoded_sub = encoded_sub.drop(object_columns, axis=1)\n",
    "\n",
    "encoded_train = encoded_train.astype(\"float64\")\n",
    "encoded_sub = encoded_sub.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e014d0ac-64ba-4ff5-b8e6-cde477709c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Utilities_NoSeWa', 'Condition2_RRAe', 'Condition2_RRAn',\n",
       "       'Condition2_RRNn', 'HouseStyle_2.5Fin', 'RoofMatl_Membran',\n",
       "       'RoofMatl_Metal', 'RoofMatl_Roll', 'Exterior1st_ImStucc',\n",
       "       'Exterior1st_Stone', 'Exterior2nd_Other', 'Heating_OthW',\n",
       "       'Electrical_Mix', 'PoolQC_Fa', 'MiscFeature_TenC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sub.columns[(encoded_sub == 0).all()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91ea72-db35-4615-ab5a-a89359856bae",
   "metadata": {},
   "source": [
    "In the test set there are no objects which have the above features set to one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e4763-671c-4194-aa8f-e5b67dac9b0d",
   "metadata": {},
   "source": [
    "## Anomaly objects, low-informational\n",
    "\n",
    "To delete anomalies we will use simple 3-sigma rule for the continuous features. To delete less objects we firstly use even 4 sigma interval.\n",
    "\n",
    "Then we will filter our data from non-informative feature with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e03471-a08a-4e0e-b334-c1bbd4f62ba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT FILTERED OBJECTS 269\n",
      "SHAPE AFTER ANOMALY FILTERING (1190, 271)\n",
      "SHAPE AFTER LOW-VARIANCE FEATURES DELETING (1190, 176)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1459, 176)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sub = encoded_sub.copy().drop([\"Id\"], axis=1)\n",
    "X = encoded_train.drop([\"SalePrice\"], axis=1).copy().drop([\"Id\"], axis=1)\n",
    "y = encoded_train[\"SalePrice\"].copy()\n",
    "\n",
    "filtered_X = X.copy()\n",
    "filtered_X[\"is_filtered\"] = pd.Series(np.zeros(filtered_X.shape[0]), dtype=\"int64\")\n",
    "\n",
    "# mark outliers according to \"4 sigma\" rule\n",
    "for column in continuous_columns:\n",
    "    \n",
    "    col_values = filtered_X[column]\n",
    "    mean = col_values.mean()\n",
    "    std = col_values.std()\n",
    "    filtered_X.loc[(mean - 4*std > col_values) | (col_values > mean + 4*std), \"is_filtered\"] = 1\n",
    "\n",
    "print(\"CNT FILTERED OBJECTS\", filtered_X[\"is_filtered\"].sum())\n",
    "\n",
    "# deleting outliers\n",
    "X = filtered_X[filtered_X[\"is_filtered\"] == 0].copy().drop(\"is_filtered\", axis=1)\n",
    "y = y[filtered_X[\"is_filtered\"] == 0].copy()\n",
    "\n",
    "print(\"SHAPE AFTER ANOMALY FILTERING\", X.shape)\n",
    "\n",
    "'''\n",
    "    After deleting some objects it can be that there are some features now which have only\n",
    "    one value or some value near constant. It means that their std is equal to zero or almost equal \n",
    "    to zero\n",
    "'''\n",
    "\n",
    "stds = X.std()\n",
    "low_variance_columns = stds[stds < 0.1].index.values\n",
    "X_sub = X_sub.drop(low_variance_columns, axis=1)\n",
    "X = X.drop(low_variance_columns, axis=1)\n",
    "\n",
    "print(\"SHAPE AFTER LOW-VARIANCE FEATURES DELETING\", X.shape)\n",
    "X_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4d97f-13f8-4011-a8a7-b6b3d28d5ad4",
   "metadata": {},
   "source": [
    "## Deleting correlated feature\n",
    "\n",
    "Let's delete all features with correlationg above 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f39102fb-e60f-4882-975d-9cdc3e656c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE AFTER CORRELATED WITH 0.7 THRESHOLD DELETING (1190, 136)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1459, 136)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Delete correlated with threshold 0.6\n",
    "tr = DropCorrelatedFeatures(variables=None, method='pearson', threshold=0.7)\n",
    "X1 = tr.fit_transform(X)\n",
    "X1_sub = tr.transform(X_sub)\n",
    "print(\"SHAPE AFTER CORRELATED WITH 0.7 THRESHOLD DELETING\", X1.shape)\n",
    "X1_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce076a-e9fc-47ef-a091-eb94d664aa3b",
   "metadata": {},
   "source": [
    "## Model applying\n",
    "\n",
    "Now we have data without anomalies (in terms of 4 sigmas) and all features with correlations more than 0.7 were deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c11a461d-f7b1-4db4-958d-052be0224c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(952, 136)\n",
      "TRAIN LOG RMSE: 0.09168057743275664\n",
      "TEST LOG RMSE: 0.10265950723222719\n",
      "...................................\n",
      "TRAIN RMSE 17027.966490021885\n",
      "TEST RMSE 17761.881040230197\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=1)\n",
    "\n",
    "#Select all remaining continuous features\n",
    "continuous_features = train.select_dtypes(exclude=\"object\").drop([\"SalePrice\", \"Id\", \"MSSubClass\"], axis=1)\n",
    "continuous_features = continuous_features.drop(tr.features_to_drop_, axis=1, errors=\"ignore\")\n",
    "continuous_features = continuous_features.drop(low_variance_columns, axis=1, errors=\"ignore\")\n",
    "continuous_features = continuous_features.columns.values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[continuous_features] = scaler.fit_transform(X_train[continuous_features])\n",
    "X_test[continuous_features] = scaler.transform(X_test[continuous_features])\n",
    "\n",
    "real_y_train = y_train.copy()\n",
    "real_y_test = y_test.copy()\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "reg1 = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "train_pred = reg1.predict(X_train)\n",
    "test_pred = reg1.predict(X_test)\n",
    "\n",
    "print(\"TRAIN LOG RMSE:\", root_mean_squared_error(y_train, train_pred))\n",
    "print(\"TEST LOG RMSE:\", root_mean_squared_error(y_test, test_pred))\n",
    "print(\".\" * 35)\n",
    "print(\"TRAIN RMSE\", root_mean_squared_error(real_y_train, np.exp(train_pred)))\n",
    "print(\"TEST RMSE\", root_mean_squared_error(real_y_test, np.exp(test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113badc3-3f11-4fd8-8f2d-95594f282c5a",
   "metadata": {},
   "source": [
    "This time we see that our OLS model doesn't give us large values on the test set. That's because we have deleted correlated features.\n",
    "And cleared our data from features with constant values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130df0e3-1ae0-40fd-8658-836c62c2e965",
   "metadata": {},
   "source": [
    "Let's check the highest coefficients in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9494286c-4406-42f7-9385-48d28432902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GrLivArea................ 135431081302.48347\n",
      "2ndFlrSF................. -124579564626.18889\n",
      "1stFlrSF................. -103724018763.79666\n",
      "LowQualFinSF............. -2171672710.745196\n",
      "Functional_Typ........... 0.16978049278259277\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_idx = np.argsort(-np.abs(reg1.coef_))\n",
    "coef = reg1.coef_[sorted_coef_idx]\n",
    "for i in range(0, 5):\n",
    "    print(\"{:.<025} {}\".format(X_train.columns[sorted_coef_idx[i]], coef[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7958ab4-12e5-4298-8d20-3f53f330e282",
   "metadata": {},
   "source": [
    "Once again we received large coefficients. It seems like we have deleted not enough correlated features. Moreover there is a problem that if we rerun our notebook with correlational threshold as 0.6 (if we make the threshold lower) we will receive normal coefficients but the model will have much worse results on a train and test set. Let's show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37f60a03-a549-49e9-ae46-8ff8e6152bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE AFTER CORRELATED WITH 0.6 THRESHOLD DELETING (1190, 122)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1459, 122)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Delete correlated with threshold 0.6\n",
    "tr2 = DropCorrelatedFeatures(variables=None, method='pearson', threshold=0.6)\n",
    "X2 = tr2.fit_transform(X)\n",
    "X2_sub = tr2.transform(X_sub)\n",
    "\n",
    "print(\"SHAPE AFTER CORRELATED WITH 0.6 THRESHOLD DELETING\", X2.shape)\n",
    "X2_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dadb741-1d2f-4255-904b-2c3dba84035f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOG RMSE: 0.10360826409008214\n",
      "TEST LOG RMSE: 0.1039663528019374\n",
      "...................................\n",
      "TRAIN RMSE 19370.099182081205\n",
      "TEST RMSE 19226.562372625296\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state=1)\n",
    "\n",
    "#Select all remaining continuous features\n",
    "continuous_features = train.select_dtypes(exclude=\"object\").drop([\"SalePrice\", \"Id\", \"MSSubClass\"], axis=1)\n",
    "continuous_features = continuous_features.drop(tr2.features_to_drop_, axis=1, errors=\"ignore\")\n",
    "continuous_features = continuous_features.drop(low_variance_columns, axis=1, errors=\"ignore\")\n",
    "continuous_features = continuous_features.columns.values\n",
    "\n",
    "X_train[continuous_features] = scaler.fit_transform(X_train[continuous_features])\n",
    "X_test[continuous_features] = scaler.transform(X_test[continuous_features])\n",
    "\n",
    "real_y_train = y_train.copy()\n",
    "real_y_test = y_test.copy()\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)\n",
    "\n",
    "# Train model on new set of features\n",
    "reg2 = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "train_pred = reg2.predict(X_train)\n",
    "test_pred = reg2.predict(X_test)\n",
    "\n",
    "print(\"TRAIN LOG RMSE:\", root_mean_squared_error(y_train, train_pred))\n",
    "print(\"TEST LOG RMSE:\", root_mean_squared_error(y_test, test_pred))\n",
    "print(\".\" * 35)\n",
    "print(\"TRAIN RMSE\", root_mean_squared_error(real_y_train, np.exp(train_pred)))\n",
    "print(\"TEST RMSE\", root_mean_squared_error(real_y_test, np.exp(test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf235bd6-73bf-4e98-88ae-94acdd97fbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning_FV.............. 0.172044164477254\n",
      "Neighborhood_StoneBr..... 0.16847152546880867\n",
      "Exterior1st_Stucco....... 0.1599256414534786\n",
      "ExterCond_Fa............. -0.15860989511372817\n",
      "Exterior1st_BrkFace...... 0.15379551246096257\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_idx = np.argsort(-np.abs(reg2.coef_))\n",
    "coef = reg2.coef_[sorted_coef_idx]\n",
    "for i in range(0, 5):\n",
    "    print(\"{:.<025} {}\".format(X_train.columns[sorted_coef_idx[i]], coef[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3869e4-6488-4942-b292-4af815ade85a",
   "metadata": {},
   "source": [
    "It means that if we delete not enough correlated coefficients we receive overfitting and if we delete them enough we receive underfitting.\n",
    "\n",
    "What can be the solution here? We can try to find balance between deleting of correlated features and the model performance.\n",
    "Even in terms of the simple linear regression it's not a trivial task.\n",
    "\n",
    "If we had all features not correlated we would just look at which features have big coefficients and which have not and decide which features are important and which are not.\n",
    "\n",
    "But if we have correlated features we can't say from the model's coefficients if they are important or not because we don't know if the feature has large coefficient because it is important or because it's correlated with some other feature.\n",
    "\n",
    "For the beginning I prefer not to do this time-consuming work and solve it in the future notebooks with regularization\n",
    "\n",
    "Anyway let's make a submission with our second model which has stable coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d855ab8-d7c0-48a9-a422-7d2486c8d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub_X = X2_sub\n",
    "\n",
    "continuous_features = train.select_dtypes(exclude=\"object\").drop([\"SalePrice\", \"Id\", \"MSSubClass\"], axis=1)\n",
    "continuous_features = continuous_features.drop(tr2.features_to_drop_, axis=1, errors=\"ignore\")\n",
    "continuous_features = continuous_features.drop(low_variance_columns, axis=1, errors=\"ignore\")\n",
    "continuous_features = continuous_features.columns.values\n",
    "\n",
    "final_sub_X[continuous_features] = scaler.transform(final_sub_X[continuous_features])\n",
    "\n",
    "submission = pd.Series(np.exp(reg2.predict(final_sub_X)))\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df[\"Id\"] = sub[\"Id\"]\n",
    "submission_df[\"SalePrice\"] = submission\n",
    "\n",
    "submission_df.to_csv(\"submissions/ols/ols_correlations_0.6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c436852-81c3-464b-8e33-082a9df27e66",
   "metadata": {},
   "source": [
    "We have used not the whole dataset for training because when we use the whole dataset it turns out that not-splitted dataset gives us obverfitting with high coefficients.\n",
    "\n",
    "This solution gives us worse results in submission to kaggle: **0.17688**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0a99e-56ab-41a5-a8a0-beae1825c2e9",
   "metadata": {},
   "source": [
    "This will be a lesson for us that simple Linear Regression has many problems which are solved not easly with only data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e20cc-f744-4dd8-8709-ac20ec463053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
