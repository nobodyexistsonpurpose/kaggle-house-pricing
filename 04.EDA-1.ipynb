{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/filled/train.csv\")\n",
    "test = pd.read_csv(\"data/filled/test.csv\")\n",
    "\n",
    "encoded_train = pd.read_csv(\"data/encoded/train.csv\")\n",
    "encoded_test = pd.read_csv(\"data/encoded/test.csv\")\n",
    "\n",
    "scaled_train = pd.read_csv(\"data/scaled/train.csv\")\n",
    "scaled_test = pd.read_csv(\"data/scaled/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "In this notebook we will do some EDA with our data\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#sale-price\">Sale Price</a></li>\n",
    "    <li><a href=\"#continuous\">Continuous Features</a>\n",
    "        <ul>\n",
    "            <li><a href=\"#correlations\">Correlations</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><a href=\"#pca-visualization\">PCA analysis and visualization</a>\n",
    "        <ul>\n",
    "            <li><a href=\"#one-dimension\">One Dimension</a></li>\n",
    "            <li><a href=\"#two-dimensions\">Two Dimensions</a></li>\n",
    "            <li><a href=\"#three-dimensions\">Three Dimensions</a></li>\n",
    "            <li><a href=\"#most-affecting-pca\">Checking the most affecting features in PCA</a></li>\n",
    "            <li><a href=\"#comparing-pca-train-test\">Comparing PCA visualizations for train and test</a></li>\n",
    "            <li><a href=\"#pca-data-saving\">Saving PCA data</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><a href=\"#anomaly-detection\">Anomaly detection</a>\n",
    "        <ul>\n",
    "            <li><a href=\"#3-sigma-rule-pca\">3 sigma rule for PCA</a></li>\n",
    "            <li><a href=\"#isolation-forest\">Isolation Forest</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><a href=\"#feature-selection\">Feature Selection</a>\n",
    "        <ul>\n",
    "            <li><a href=\"#mutual-information\">Mutual Information gain</a></li>\n",
    "            <li><a href=\"#correlational-analysis\">Correlational analysis</a></li>\n",
    "            <li><a href=\"#random-forest-feature-importance\">Random Forest feature importance</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><a href=\"#cluster-analysis\">Cluster Analysis</a>\n",
    "        <ul>\n",
    "            <li><a href=\"#k-means-optimal-k\">K-Means optimal number of clusters</a></li>\n",
    "            <li><a href=\"#clustering-k-2\">Clustering with K=2</a></li>\n",
    "            <li><a href=\"#clustering-k-3\">Clustering with K=3</a></li>\n",
    "            <li><a href=\"#independent-cluster-pca\">Independent Cluster PCA tranformation</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "<a id=\"sale-price\"></a>\n",
    "## SalePrice\n",
    "\n",
    "All begins with the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(train, x=\"SalePrice\", title=\"SalePrice histogram\")\n",
    "fig.update_layout(width=1000, height=500, bargap=0.1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "We see that our target has long tail. BUT the tail is small in terms of object amount in it. There are only 9 objects with price which is higher than 500K. On this purpose we have already made a logarithmic transformation to our target during scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "<a id=\"continuous\"></a>\n",
    "## Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns = train.select_dtypes(exclude=\"object\").drop([\"Id\", \"MSSubClass\"], axis=1)\n",
    "continuous_columns.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "<a id=\"correlations\"></a>\n",
    "## Correlations\n",
    "\n",
    "First of all let's look at the correlations of the continuous features including the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = continuous_columns.corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Let's check the highest and the lowest correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_corr = corr.mask(mask, 0).unstack().sort_values(ascending=False)\n",
    "print(\"TOP 10 HIGH CORRELATIONS\\n\", top_corr.head(10))\n",
    "print(\"TOP 10 LOW CORRELATIONS\\n\", top_corr.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "We see that there are 6 pairs off features which have correlations greater than 0.7 and 2 pairs which have correlations lower than -0.4. This means that our dataset is high-correlated and models such simple linear regression can have some difficults during training on it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "<a id=\"pca-visualization\"></a>\n",
    "## PCA analysis and visualization\n",
    "\n",
    "Let's look at the first several components of PCA transformation of our data. We can check if our data is splitted into several easy distinguishable areas and how price of the houses are distributed among them.\n",
    "\n",
    "Also we can check possible outliers and if the test data looks corresponding to the train one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "N_components = 50\n",
    "\n",
    "pca = PCA(n_components = N_components, random_state=RANDOM_STATE)\n",
    "\n",
    "pca_train = pca.fit_transform(scaled_train.drop([\"SalePrice\"], axis=1))\n",
    "\n",
    "print(\"WHOLE VARIANCE\", scaled_train.drop([\"SalePrice\"], axis=1).var().sum())\n",
    "print(\"EXPLAINED VARIANCE\", pca.explained_variance_)\n",
    "print(\"EXPLAINED VARIANCE RATIO\", pca.explained_variance_ratio_)\n",
    "print(\"FIRST 50 DIMENSION EXPLAINED VARIANCE\", pca.explained_variance_ratio_.sum())\n",
    "\n",
    "pca_train = pd.DataFrame(pca_train, columns=[\"pc{}\".format(i) for i in range(1, 51)])\n",
    "pca_train[\"SalePrice\"] = scaled_train[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "We see that in our dataset there are no principal components which can describe the most part of the variance of the whole dataset since even the first 5 principal components describe only 37% of the variance.\n",
    "The first 50 Principal components describe almost 88.54% of the whole variance\n",
    "\n",
    "Anyway we can plot our first three \"main\" dimensions of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "<a id=\"one-dimension\"></a>\n",
    "## One Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(pca_train, x=\"pc1\",\n",
    "                 y=[0 for i in range(pca_train.shape[0])],\n",
    "                color=\"SalePrice\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Even for the first principal component we see that our sale price is increasing with increasing of the first components. In addition we see 2 values (on the right side) which obviously fall out of the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "<a id=\"two-dimensions\"></a>\n",
    "## Two Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(pca_train, x=\"pc1\", y=\"pc2\", color=\"SalePrice\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "That's interesting because we see that along the second principal component sale price almost doesn't change. It changes along the first component but almost the same along the second one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "<a id=\"three-dimensions\"></a>\n",
    "## Three dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pca_train, x=\"pc1\", y=\"pc2\", z=\"pc3\", color=\"SalePrice\")\n",
    "fig.update_traces(marker_size = 3)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Along the third component our sale price also doesn't change.\n",
    "\n",
    "In addition it's very important to notice that our dataset seems to consist of two areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Let's also plot dependence of SalePrice from pc1 and pc2 in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pca_train, x=\"pc1\", y=\"pc2\", z=\"SalePrice\", color=\"SalePrice\")\n",
    "fig.update_traces(marker_size = 3)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Here we see almost clear linear dependency of SalePrice from pc1 and pc2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "<a id=\"most-affecting-pca\"></a>\n",
    "## Checking the most affecting features in PCA\n",
    "\n",
    "We can look which original features affect the most each of the first 10 principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "\n",
    "principal_components = pca.components_\n",
    "\n",
    "top_coeffs = np.sort(np.absolute(principal_components), axis=1)[:5, -N:]\n",
    "top_features_idx = np.argsort(np.absolute(principal_components), axis=1)[:5, -N:]\n",
    "\n",
    "print(\"TOP {} MOST AFFECTING FEATURES FOR EACH PC\".format(N))\n",
    "for ii, row in enumerate(top_features_idx):\n",
    "    print(\"PC_{}\".format(ii + 1))\n",
    "    print(\"features:\", np.flip(scaled_train.drop([\"SalePrice\"], axis=1).columns.values[row]))\n",
    "    print(\"coefficients (abs):\", np.flip(top_coeffs[ii]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "We see that the most valuable features for the first principal component are **OverallQual**, **GarageCars**, **GrLivArea**, **GarageArea** and **YearBuilt**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "<a id=\"comparing-pca-train-test\"></a>\n",
    "## Comparing PCA visualizations for train and test\n",
    "\n",
    "We can check if our scatter plots for train and test look similar after PCA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_test = pca.transform(scaled_test)\n",
    "\n",
    "print(\"WHOLE VARIANCE\", scaled_test.var().sum())\n",
    "\n",
    "pca_test = pd.DataFrame(pca_test, columns=[\"pc{}\".format(i) for i in range(1, 51)])\n",
    "\n",
    "fig = px.scatter_3d(pca_test, x=\"pc1\", y=\"pc2\", z=\"pc3\")\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "We have pretty similar plot which means that train and test set has some similarity in their structure. That's pretty good because that means that trained on the train set models which perform well on it can perform also not bad on the test set (perhaps :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## PCA data saving\n",
    "\n",
    "Let's save our train and test dataset but with PCA transformation. We will use the first 50 principal components of our train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train.to_csv(\"data/pca/train50.csv\", index=False)\n",
    "pca_test.to_csv(\"data/pca/test50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "<a id=\"anomaly-detection\"></a>\n",
    "## Anomaly detection\n",
    "\n",
    "Now we will use some approaches for anomaly detection. Generally we can manually decide which train points seem to be anomaly based on the above PCA visualization but we will apply some general approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## 3 Sigma rule on PCA\n",
    "\n",
    "As the most easy step we can try to use simple 3 sigma rule on our PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_train = pca_train.copy()\n",
    "\n",
    "sigma_train[\"is_filtered\"] = pd.Series(np.zeros(sigma_train.shape[0]), dtype=\"int64\")\n",
    "\n",
    "for column in sigma_train.columns.drop(\"SalePrice\").values:\n",
    "    \n",
    "    values = sigma_train[column]\n",
    "    mean = values.mean()\n",
    "    std = values.std()\n",
    "    \n",
    "    sigma_train.loc[(mean - 3*std > values) | (values > mean + 3*std), \"is_filtered\"] = 1\n",
    "\n",
    "print(\"CNT FILTERED OBJECTS\", sigma_train[\"is_filtered\"].sum())\n",
    "\n",
    "fig = px.scatter_3d(pca_train, x=\"pc1\", y=\"pc2\", z=\"SalePrice\", color=sigma_train[\"is_filtered\"])\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "3 sigma rule throws out to much objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "<a id=\"isolation-forest\"></a>\n",
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "isolated_train = pca_train.copy()\n",
    "\n",
    "clf = IsolationForest(random_state=3, contamination=0.05).fit(isolated_train)\n",
    "isolated_train[\"anomaly_score\"] = clf.predict(isolated_train)\n",
    "\n",
    "print(\"COUNT OF ANOMALIES:\", isolated_train[isolated_train[\"anomaly_score\"] == -1].shape[0])\n",
    "\n",
    "fig = px.scatter_3d(pca_train, x=\"pc1\", y=\"pc2\", z=\"SalePrice\", color=isolated_train[\"anomaly_score\"])\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Isolation Forest throw out some objects which are too far from the main area of dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "<a id=\"feature-selection\"></a>\n",
    "## Feature selection\n",
    "\n",
    "We've done feature extraction using the PCA and have noticed some interesting pattern that the first principal components mostly shows us changes in the target feature.\n",
    "\n",
    "Let's use some feature selection techniques to see which features give us useful information and which do not.\n",
    "\n",
    "We will not delete features in this notebook because I think that we have done enough manipulation to our data for the future modeling. We will check feature informativeness just to make some additional investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "<a id=\"mutual-information\"></a>\n",
    "### Mutual Information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_train = encoded_train.drop([\"SalePrice\"], axis=1)\n",
    "target = encoded_train[\"SalePrice\"]\n",
    "\n",
    "mis = mutual_info_regression(mutual_train, target)\n",
    "\n",
    "N = 10\n",
    "\n",
    "\n",
    "topNgain = np.flip(np.sort(mis)[-N:])\n",
    "topNfeatures = np.flip(mutual_train.columns.values[np.argsort(mis)[-N:]])\n",
    "\n",
    "botNfeatures = mutual_train.columns.values[np.argsort(mis)[0:N]]\n",
    "botNgain = np.sort(mis)[0:N]\n",
    "\n",
    "print(\"TOP MUTUAL GAIN FEATURES:\", topNfeatures)\n",
    "print(\"THEIR MUTUAL GAIN:\", topNgain)\n",
    "print(\"BOTTOM MUTUAL GAIN FEATURES\", botNfeatures)\n",
    "print(\"THEIR MUTUAL GAIN\", botNgain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_series = pd.Series(mis, index=mutual_train.columns.values)\n",
    "\n",
    "print(\"Count of features with mutual gain less than 1e-5:\", mis_series[mis_series < 1e-5].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "The most interesting thing which we can notice is that in our top-10 mutual information gain features there are features which were also in the top-5 most affecting features for the first principal component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "<a id=\"correlational-analysis\"></a>\n",
    "### Correlational analysis\n",
    "\n",
    "Correlations analysis we have done above in the second stage of EDA. It can be used for understanding which features can be usefull for the linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "<a id=\"random-forest-feature-importance\"></a>\n",
    "### Random Forest feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = encoded_train.drop([\"SalePrice\"], axis=1)\n",
    "y = encoded_train[\"SalePrice\"]\n",
    "\n",
    "forest = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "forest.fit(X, y)\n",
    "features_df = pd.DataFrame(forest.feature_importances_, columns=[\"importance\"], index=X.columns)\n",
    "features_df.sort_values(by=\"importance\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Random Forest makes almost all splits to increase informativity along the OverallQual component. We see that after the 2ndFlrSF feature other features are used less then in 3% cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[features_df[\"importance\"] < 1e-3].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "There are 278 features with importance less than 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "<a id=\"cluster-analysis\"></a>\n",
    "## Cluster Analysis\n",
    "\n",
    "First of all. Let's look at our PCA visualization not through the first two principal components, but through the second and third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pca_train, x=\"pc2\", y=\"pc3\", z=\"SalePrice\", color=\"SalePrice\")\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "What we see? We see that there are no so beautiful linear picture as we saw before. It looks like there are two areas. One locates higher with respect to Sale Price and another locates lower. \n",
    "\n",
    "Moreover if we take a look at the visualization using the 4th and 5th principal components we can also find out that our data is obviously splitted for two other areas. It is interesting for us that the second (smaller) area consists of low-priced houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pca_train, x=\"pc4\", y=\"pc5\", z=\"SalePrice\", color=\"SalePrice\")\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "These thoughts give us a reason to make some clustering to our data to find some separation of the dataset into some areas which have different properties in terms of predicting the target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "<a id=\"k-means-optimal-k\"></a>\n",
    "## K-Means optimal number of clusters\n",
    "\n",
    "Let's use the widely known K-means algorithm and try to find optimal count of clusters in our data. First of all we will use the Elbow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_train = scaled_train.drop([\"SalePrice\"], axis=1).copy() \n",
    "root_of_sum_of_squared_distances = []\n",
    "K = range(2,30)\n",
    "\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k, random_state=RANDOM_STATE)\n",
    "    km = km.fit(clustered_train)\n",
    "    root_of_sum_of_squared_distances.append(km.inertia_**(1/2))\n",
    "\n",
    "plt.plot(K, root_of_sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('root_of_sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "From the obtained plot it's not so obvious where is the optimal number of clusters. It seems the sum of squared distances is going down not smoothly. Let's calculate the silhouette score for our clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "for k in K:\n",
    "\n",
    "    km = KMeans(n_clusters=k, random_state=2)\n",
    "    cluster_labels = km.fit_predict(clustered_train)\n",
    "    \n",
    "    # Рассчитываем средний коэффициент силуэта для текущего K\n",
    "    silhouette_avg = silhouette_score(clustered_train, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "plt.plot(K, silhouette_scores, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('silhouette_scores')\n",
    "plt.title('Silhouette Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "After K=3 our silhouette score strictly goes down. But generally even for K=2 and K=3 it's quite small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "<a id=\"clustering-k-2\"></a>\n",
    "## Clustering with K=2\n",
    "\n",
    "Since all scores have values less than 0.12 and graph from elbow method goes down smoothly it seem's that K-means cannot give us some good clustering\n",
    "\n",
    "Let's check how our data looks after the clastering with K=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "km2 = KMeans(n_clusters=2, random_state=2)\n",
    "km2_labels = km2.fit_predict(clustered_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pca_train, x=\"pc1\", y=\"pc2\", z=\"pc3\", color=km2_labels)\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "With K=2 the K-means algorithm splits our dataset into two clasters with different kind of sale prices. We have noticed these arease before when we made the first plot of our data in PCA coordinates\n",
    "The blue area has higher sale prices than the yellow one.\n",
    "\n",
    "It means that there is some natural metric difference between objects with high price and low price. Probably we can use this information in futhure modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "<a id=\"clustering-k-3\"></a>\n",
    "## Clustering with K=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "km3 = KMeans(n_clusters=3, random_state=2)\n",
    "km3_labels = km3.fit_predict(clustered_train)\n",
    "\n",
    "fig = px.scatter_3d(pca_train, x=\"pc2\", y=\"pc3\", z=\"SalePrice\", color=km3_labels)\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "With respect to pc2 and pc3 we cannot unterstand why k-means gives us exactly this kind of cluster splitting. But if we look at our data from the side of pc4 and pc5, than"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pca_train, x=\"pc4\", y=\"pc5\", z=\"SalePrice\", color=km3_labels)\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "We see one more natural split in our data. One cluster is for high-valued houses. Two others are with low-valued houses but as we see the low-values houses seem also to be splitted into two different groups. \n",
    "\n",
    "Because of the fact that we can split \"blue\" group of the other two by just the coordinate pc3 that means that for example desicion tree can see this kind of splitting. Our second split between high-valued houses and low-valued houses also can be spoted by simple Desicion Tree or Random Forest because we saw that SalePrice are highly changing along the pc1 coordinate which can be used by Decision Tree.\n",
    "\n",
    "But these are only reasonings for the Decision Trees. It also can be that in different clusters the data has different behaviour and can be modeled with different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "<a id=\"independent-cluster-pca\"></a>\n",
    "## Independent Cluster PCA tranformation\n",
    "\n",
    "Let's try to do one more thing. Perhaps it makes sense to cluster our dataset and only them use PCA independently to each cluster.\n",
    "The reason for this can be that there can be a situation when we will receive much difference in Principal components in each cluster.\n",
    "This means that data in each cluster has some independent patterns from the point of PCA view.\n",
    "\n",
    "We will do our visualization for K=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.Series(km3_labels)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "cluster1 = scaler.fit_transform(encoded_train[clusters == 0].drop(\"SalePrice\", axis=1))\n",
    "cluster2 = scaler.fit_transform(encoded_train[clusters == 1].drop(\"SalePrice\", axis=1))\n",
    "cluster3 = scaler.fit_transform(encoded_train[clusters == 2].drop(\"SalePrice\", axis=1))\n",
    "\n",
    "pca1 = PCA(n_components = 50, random_state=RANDOM_STATE)\n",
    "pca2 = PCA(n_components = 50, random_state=RANDOM_STATE)\n",
    "pca3 = PCA(n_components = 50, random_state=RANDOM_STATE)\n",
    "\n",
    "pca_cluster1 = pca1.fit_transform(cluster1)\n",
    "pca_cluster2 = pca2.fit_transform(cluster2)\n",
    "pca_cluster3 = pca3.fit_transform(cluster3)\n",
    "\n",
    "pca_cluster1 = pd.DataFrame(pca_cluster1, columns=[\"pc{}\".format(i) for i in range(1, 51)])\n",
    "pca_cluster2 = pd.DataFrame(pca_cluster2, columns=[\"pc{}\".format(i) for i in range(1, 51)])\n",
    "pca_cluster3 = pd.DataFrame(pca_cluster3, columns=[\"pc{}\".format(i) for i in range(1, 51)])\n",
    "\n",
    "pca_cluster1[\"SalePrice\"] = scaled_train[clusters == 0][\"SalePrice\"].values\n",
    "pca_cluster2[\"SalePrice\"] = scaled_train[clusters == 1][\"SalePrice\"].values\n",
    "pca_cluster3[\"SalePrice\"] = scaled_train[clusters == 2][\"SalePrice\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "## Explained variance in each cluster after PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EXPLAINED VARIANCE RATIO CLUSTER 1\", pca1.explained_variance_ratio_[:5])\n",
    "print(\"EXPLAINED VARIANCE RATIO CLUSTER 2\", pca2.explained_variance_ratio_[:5])\n",
    "print(\"EXPLAINED VARIANCE RATIO CLUSTER 3\", pca3.explained_variance_ratio_[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "## Distance between first principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PC1 distance between cluster 1 and 2:\", norm(pca1.components_[0] - pca2.components_[0]))\n",
    "print(\"PC1 distance between cluster 2 and 3:\", norm(pca2.components_[0] - pca3.components_[0]))\n",
    "print(\"PC1 distance between cluster 3 and 4:\", norm(pca3.components_[0] - pca1.components_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pca_cluster1, x=\"pc1\", y=\"pc2\", z=\"SalePrice\", color=\"SalePrice\")\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pca_cluster2, x=\"pc1\", y=\"pc2\", z=\"SalePrice\", color=\"SalePrice\")\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pca_cluster3, x=\"pc1\", y=\"pc2\", z=\"SalePrice\", color=\"SalePrice\")\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "For now I don't see some obvious application of such a clustering and transformation. It's hard to say if some well-known model can perform good on this data. \n",
    "\n",
    "We must spend more time for cluster analysis because we have used only one algorithm. Moreover K-means can give us different results based on the random_state parameter. Perhaps there are some more cool clustering but we didn't get it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "This EDA file is large and I prefer to continue cluster analysis in another new notebook\n",
    "\n",
    "The first EDA file is finished..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-environment",
   "language": "python",
   "name": "ml-environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
