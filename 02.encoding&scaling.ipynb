{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have data without missing values\n",
    "train = pd.read_csv(\"data/filled/train.csv\")\n",
    "test = pd.read_csv(\"data/filled/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Features' dtypes\n",
    "\n",
    "First of all we need to understand if features' dtypes really corresponds to real features' meaning and if there is no mistake in dtype for a particular feature.\n",
    "\n",
    "To ensure that all is OK we need to check data_description.txt file.\n",
    "\n",
    "Now let's separate object features from numerical ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = train.select_dtypes(include='object')\n",
    "continuous_columns = train.select_dtypes(exclude=\"object\").drop([\"SalePrice\", \"Id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "With all categorical features there are no problem. They are really categorical. The only interesting thing is with features which have some ordering meaning like PoolQC or GarageQual. These features can be encoded with LabelEncoder and not with One Hot Encoding. But as the first version of encoding we will encode almost all categorical features with one hot except the features which have only two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "With continuous features there are several features which are categorical but were encoded in the data like numbers and thus pandas think they are continuous. \n",
    "\n",
    "* **MsSubClass** &ndash; 100% categorical feature\n",
    "* **OverallQual** &ndash; Also a categorical feature BUT can be interpretet as already encoded feature using label encoding and thus we will not encode it\n",
    "* **OverallCond** &ndash; The same situation as with OverallQual\n",
    "\n",
    "So we need to add MsSubClass feature to our object columns and remove it from the continuous ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns[\"MSSubClass\"] = continuous_columns[\"MSSubClass\"]\n",
    "continuous_columns = continuous_columns.drop([\"MSSubClass\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Category features encoding\n",
    "\n",
    "Let's check how many category features we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "It can be that the train and test sets have not the same set of categories in the same column (for example the test set can have some categories which the train set doesn't have). On this purpose we will stack train and test sets vertically together, then encode them and then split them back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train.drop(\"SalePrice\", axis=1), test], axis=0, ignore_index=True)\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "category_counts = [data[column].value_counts().shape[0] for column in object_columns.columns]\n",
    "print(sum(category_counts))\n",
    "print(len(category_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "We see that if we use, for example, one hot encoding, we will have additional 281 columns in our data (minus 44 because we will delete previous columns of taken feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = data.copy()\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot_encoded = encoder.fit_transform(encoded[object_columns.columns])      \n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(object_columns.columns))\n",
    "encoded = pd.concat([encoded, one_hot_df], axis=1)\n",
    "encoded = encoded.drop(object_columns.columns, axis=1)\n",
    "\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train = encoded.iloc[:1459].copy()\n",
    "encoded_train[\"SalePrice\"] = train[\"SalePrice\"]\n",
    "\n",
    "encoded_test = encoded.iloc[1459:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_train.shape)\n",
    "print(encoded_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Casting continuous features to float\n",
    "\n",
    "To work with all features we need to convert them to float. Because of the fact that we have encoded all categorical features now we have all our features numerical. But still some of them can be integer rather than float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train = encoded_train.astype(\"float64\")\n",
    "encoded_test = encoded_test.astype(\"float64\")\n",
    "\n",
    "encoded_train = encoded_train.drop([\"Id\"], axis=1)\n",
    "encoded_test = encoded_test.drop([\"Id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train.to_csv(\"data/encoded/train.csv\", index=False)\n",
    "encoded_test.to_csv(\"data/encoded/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "In addition in this notebook we will create files with already scaled continuous features. Just not to do that in future notebooks.\n",
    "\n",
    "We scale all of the continuous columns as well as we do logarithmic transformation to the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train = encoded_train.copy() \n",
    "scaled_test = encoded_test.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "columns2scale = continuous_columns.columns\n",
    "\n",
    "scaled_train[columns2scale] = scaler.fit_transform(scaled_train[columns2scale])\n",
    "scaled_test[columns2scale] = scaler.transform(scaled_test[columns2scale])\n",
    "scaled_train[\"SalePrice\"] = np.log(scaled_train[\"SalePrice\"])\n",
    "\n",
    "scaled_train.to_csv(\"data/scaled_train.csv\", index=False)\n",
    "scaled_test.to_csv(\"data/scaled_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "When we train some models in future we will need to use just **encoded_train.csv** file and not **scaled_train.csv** because during the training we need to split our WHOLE train data into train/test, then scale firstly train and then scale test using scale parameters of train to avoid data leakage.\n",
    "\n",
    "Scaled train and test from this notebook are only for the final submission to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
