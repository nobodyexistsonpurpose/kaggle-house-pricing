{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3514a346-f70e-4409-a0f0-b1913afb274f",
   "metadata": {},
   "source": [
    "## Linear models \n",
    "\n",
    "In this notebook we will test OLS, ridge and lasso regressions but with additional data preprocessing and tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceae7e81-f841-4b5d-9bc7-c0fb1822df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from IPython.core.display import HTML\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from feature_engine.selection import DropCorrelatedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "100ad498-e578-4b6b-b411-4a4ad1a578dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 313)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/train_without_nans.csv\")\n",
    "sub = pd.read_csv(\"data/test_without_nans.csv\")\n",
    "\n",
    "encoded_train = pd.read_csv(\"data/encoded_train.csv\")\n",
    "encoded_sub = pd.read_csv(\"data/encoded_test.csv\")\n",
    "\n",
    "encoded_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e4763-671c-4194-aa8f-e5b67dac9b0d",
   "metadata": {},
   "source": [
    "## Anomaly deleting\n",
    "\n",
    "To delete anomalies we will use simple 3-sigma rule for the continuous features. To delete less objects we firstly use even 4 sigma interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e03471-a08a-4e0e-b334-c1bbd4f62ba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT FILTERED OBJECTS 269\n"
     ]
    }
   ],
   "source": [
    "X = encoded_train.drop([\"SalePrice\"], axis=1).copy()\n",
    "y = encoded_train[\"SalePrice\"].copy()\n",
    "\n",
    "continuous_features = train.select_dtypes(exclude=\"object\").drop([\"Id\", \"MSSubClass\", \"SalePrice\"], axis=1)\n",
    "#print(continuous_features.columns.values)\n",
    "\n",
    "filtered_X = X.copy()\n",
    "\n",
    "filtered_X[\"is_filtered\"] = pd.Series(np.zeros(filtered_X.shape[0]), dtype=\"int64\")\n",
    "\n",
    "# mark outliers\n",
    "for column in continuous_features.columns.values:\n",
    "    \n",
    "    col_values = filtered_X[column]\n",
    "    mean = col_values.mean()\n",
    "    std = col_values.std()\n",
    "    filtered_X.loc[(mean - 4*std > col_values) | (col_values > mean + 4*std), \"is_filtered\"] = 1\n",
    "\n",
    "print(\"CNT FILTERED OBJECTS\", filtered_X[\"is_filtered\"].sum())\n",
    "\n",
    "\n",
    "# deleting outliers\n",
    "X = filtered_X[filtered_X[\"is_filtered\"] == 0].copy().drop(\"is_filtered\", axis=1)\n",
    "y = y[filtered_X[\"is_filtered\"] == 0].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299184e-72d3-4ef6-a013-fdb65114b230",
   "metadata": {},
   "source": [
    "## Deleting features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be9cc940-6b81-442e-8087-37819ced594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1190, 312)\n",
      "(1190, 202)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    After deleting it can be that there are some features now which have only\n",
    "    one value or almost constant features. It means that their std is equal to zero\n",
    "    or is near zero\n",
    "'''\n",
    "print(X.shape)\n",
    "\n",
    "stds = X.std()\n",
    "low_variance_columns = stds[stds < 0.1].index.values\n",
    "encoded_sub = encoded_sub.drop(low_variance_columns, axis=1)\n",
    "X = X.drop(low_variance_columns, axis=1)\n",
    "X.shape\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a330ba-255b-4e8b-a4a0-9664ea34ed71",
   "metadata": {},
   "source": [
    "## Correlated feature deleting\n",
    "\n",
    "As we know all linear models are affected badly by correlated features. On this purpose we will remove features with high correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ebb55bf-1789-4048-a681-612f8eebbbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1190, 115)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = DropCorrelatedFeatures(variables=None, method='pearson', threshold=0.4)\n",
    "Xt = tr.fit_transform(X)\n",
    "encoded_sub = encoded_sub.drop(tr.features_to_drop_, axis=1)\n",
    "Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "847f8060-97a7-49ed-bd4a-4b4286da69d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 115)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce076a-e9fc-47ef-a091-eb94d664aa3b",
   "metadata": {},
   "source": [
    "## Scalilng, splitting and models applying\n",
    "\n",
    "Now we have data without anomalies (in terms of 4 sigmas) and all features with correlations more than 0.5 were deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c11a461d-f7b1-4db4-958d-052be0224c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1071, 115)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Xt\n",
    "y = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "'''\n",
    "    After train/test split it can be that in train set there are\n",
    "    also features, which have only one \n",
    "'''\n",
    "#Select all remaining continuous features\n",
    "continuous_columns = continuous_features.columns.drop(tr.features_to_drop_, errors=\"ignore\")\n",
    "continuous_columns = continuous_columns.drop(low_variance_columns, errors=\"ignore\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[continuous_columns] = scaler.fit_transform(X_train[continuous_columns])\n",
    "X_test[continuous_columns] = scaler.transform(X_test[continuous_columns])\n",
    "\n",
    "real_y_train = y_train.copy()\n",
    "real_y_test = y_test.copy()\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45214f3d-309d-477c-b618-53b4d49e0b02",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfeb37dc-6499-4c8c-bf70-469da745de72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOG RMSE: 0.10840238051335337\n",
      "TEST LOG RMSE: 0.15284137685055116\n",
      "...................................\n",
      "TRAIN RMSE 21393.48208394398\n",
      "TEST RMSE 19556.890617807636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "train_pred = lr_reg.predict(X_train)\n",
    "test_pred = lr_reg.predict(X_test)\n",
    "\n",
    "print(\"TRAIN LOG RMSE:\", root_mean_squared_error(y_train, train_pred))\n",
    "print(\"TEST LOG RMSE:\", root_mean_squared_error(y_test, test_pred))\n",
    "print(\".\" * 35)\n",
    "print(\"TRAIN RMSE\", root_mean_squared_error(real_y_train, np.exp(train_pred)))\n",
    "print(\"TEST RMSE\", root_mean_squared_error(real_y_test, np.exp(test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113badc3-3f11-4fd8-8f2d-95594f282c5a",
   "metadata": {},
   "source": [
    "This time we see that our OLS model doesn't give us large values on the test set. That's because we have deleted correlated features.\n",
    "And cleared our data from features with constant values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "899117e8-af9b-430e-b76c-549e9e4c396b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIN TRAIN PREDICTION 10.966887874466359\n",
      "MIN TEST PREDICTION 11.096970078263457\n",
      "MAX TRAIN PREDICTION 13.13075311110468\n",
      "MAX TEST PREDICTION 12.768192981025756\n"
     ]
    }
   ],
   "source": [
    "print(\"MIN TRAIN PREDICTION\", np.min(train_pred))\n",
    "print(\"MIN TEST PREDICTION\", np.min(test_pred))\n",
    "print(\"MAX TRAIN PREDICTION\", np.max(train_pred))\n",
    "print(\"MAX TEST PREDICTION\", np.max(test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130df0e3-1ae0-40fd-8658-836c62c2e965",
   "metadata": {},
   "source": [
    "Let's check the lowest coefficients in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9494286c-4406-42f7-9385-48d28432902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exterior1st_BrkFace...... 0.1914548268982117\n",
      "BsmtQual_Ex.............. 0.17436230494845154\n",
      "1stFlrSF................. 0.16875755094372852\n",
      "Neighborhood_Somerst..... 0.1634797910980309\n",
      "Exterior1st_VinylSd...... 0.1569266427707396\n",
      "Neighborhood_StoneBr..... 0.1460041176551825\n",
      "Exterior1st_MetalSd...... 0.14346492916766204\n",
      "Exterior1st_Stucco....... 0.14031542848000667\n",
      "Neighborhood_SWISU....... -0.13199564899418942\n",
      "2ndFlrSF................. 0.12803352884434469\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_idx = np.argsort(-np.abs(lr_reg.coef_))\n",
    "coef = lr_reg.coef_[sorted_coef_idx]\n",
    "for i in range(0, 10):\n",
    "    print(\"{:.<025} {}\".format(X_train.columns[sorted_coef_idx[i]], coef[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7958ab4-12e5-4298-8d20-3f53f330e282",
   "metadata": {},
   "source": [
    "Coeddicients seem to be normal. But we have received even worse test results than in the previous notebook with ridge regression without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16764844-685f-408e-b56e-3f0345f23ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
